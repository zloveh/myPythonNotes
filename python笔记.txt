1.python获取网页源代码乱码问题：

    response=requests.get('http://www.quanshuwang.com/')
    #乱码出现原因：显示的编码和原始编码不一致
    #解决办法：response.encoding='gbk'，原始编码在代码中找 例：<meta http-equiv="Content-Type" content="text/html; charset=gbk">

2.正则表达式提取问题：

    2.1：
    .*? 匹配但不提取  (.*?) 匹配且提取
    reg=r'<a target="_blank" title=".*?" href="(.*?)" class="clearfix stitle">(.*?)</a>'
    novel_url_list=re.findall(reg,data)
    2.2：
    reg=r'style5\(\);</script>(.*?)<script type="text/javascript">style6'正则中括号有实际意义，所以当出现括号时，即使加了r''还要加转义    符号
    2.3：
        reg=r'g_page_config =(.*?)g_srp_loadCss'
        data=re.findall(reg,html_text,re.S)
        print(data)
        当匹配不到时可能是因为有不可见字符，加上 re.S 即可
    2.4：匹配中文
         c = '([\u4e00-\u9fa5]+)'
         re,findall(c,a)
         

3.获取返回元组多个元素其中之一的问题：

     get_Novel_Sort_List() 返回值为 ：('http://www.quanshuwang.com/book_146047.html', '幻魔猎手')类型
     如果只想拿到url,不需要i[0]
     for i in get_Novel_Sort_List():
         print(i[0])
     最好为： for novel_url,novel_name in get_Novel_Sort_List():
                  print(novel_url)
                  print(novel_name)
     拿两个变量分别接收即可
4.图片写入而不是下载

      html=requests.get('url')#图片的url
      with open ('C:\\Users\\账套\\Desktop\\xmly\\12.jpg','wb')as f:
          f.write(html.content)
5.图像识别

       百度云文字识别
       NLP
       Wolfram Mathematica(好用)  
       python tessarect-ocr pytessarect
6.URL构造

       url='https://zhuanlan.zhihu.com/api/columns/auxten/followers'
       params={params={'limit':20,'offset':20}}
       html=requests.get(self.url,params=params)

       相当于：url='https://zhuanlan.zhihu.com/api/columns/auxten/followers?limit=20&offset=20'

7.搜索框汉字转码

       from urllib import parse
       可用于搜索内容的转码
       pictureName=parse.quote('表情包')
       print(pictureName)               #输出：%E8%A1%A8%E6%83%85%E5%8C%85

       解码：parse.unquote('%E8%A1%A8%E6%83%85%E5%8C%85')

8.打包程序为可执行文件

       1.cmd 进入py程序所在的文件夹,输入pyinstaller baidu.py 回车即可，可执行文件在dist中的exe,其中build是不必要的文件，可以删掉
       2.缩小可执行文件体积：用UPX方式。首先把upx394w文件夹放入py所在的文件夹中，然后执行pyinstaller baidu.py --upx-dir upx394w

9.open()写入编码问题
       with open('C:/Users/账套/Desktop/weibo.txt','a',errors='ignore')as f:
            f.write(data1+'\n')
       加上errors='ignore'可解决'gbk' codec can't encode character '\u200b' 

10.csv写入空行问题

        with open('C:/Users/账套/Desktop/weibo.csv', 'a',newline='')as f:
            f_csv = csv.writer(f)
            f_csv.writerow(headers)
        加上newline=''即可

11.     在域名后添加robots.txt（https://www.lagou.com/robots.txt）,可查看robots协议

12.计算密集型  I/O操作

        现在电脑一般是多核的，计算机核心是cpu,一个CPU一次正能只能执行一个进程，一个进程里面是有多个线程，一个进程的内部空间是共享的，
        每个进程里面的线程都可以使用这个共享空间
 
        所谓IO密集型任务，是指磁盘IO，网络IO占主要的任务，计算量很小（如请求网页，读写文件等）
        所谓计算密集型任务，是指CPU计算占主要的任务，CPU一直处于满负荷状态（如在一个很大的列表中查找元素，复杂的加减乘除）

        python中的多线程适合IO密集型任务（爬虫），而不适合计算密集型任务，而多进程（同一时间执行的进程数量取决电脑CPU核心数）
        适合计算密集型任务

        一个python进程只有一个GIL(全局解释器)锁，python里面的线程想要运行，必须获得GIL锁

13. hasattr(object,name):
         判断一个对象里面是否有name属性或方法，返回bool值，有返回True，name属性需要用引号括起来: "name"
    getattr(object,name[.default]):
         获取对象object属性或方法，如果存在打印出来，如果不存在，打印出默认值，默认值可选，如果返回的是对象方法，打印出来的是它的内存         地址，如果需要运行这个方法，可以在后面添加一对括号
         getattr(t,"name")            #获取name属性，存在就打印出来
         getattr(t,"run")             #获取run方法，存在就打印出内存地址
         getattr(t,"run")()           #获取run方法，并运行
         getattr(t,"age","18")        #若属性不存在就返回默认值'18'
    getattr(object,name,values):
         给对象的属性赋值，若属性不存在，先创建再赋值
         setattr(t,"age","18")

14. Twisted异步编程

         14.1 在同步编程中，一个函数会先请求数据，然后等待数据，最后处理它。在异步编程中，一个函数请求数据之后，会在数据准备就绪后，让               外部库调用其callback函数。
         14.2 Twisted的并发模型：非阻塞编码或者叫异步编码
              调度通常的实现方案是：当连接准备好读或是写时，调度系统会调用一个之前注册过的函数――通常被叫做异步，事件驱动或是基于回调               的编程。
         14.3 callback（回调）是通知应用程序数据已经就绪的经典模型应用程序在调用一个方法，试图获取一些数据时，同时提供一个callback函数              ,当数据就绪时，这个 callback函数会被调用，并且就绪的数据就是调用的参数之一。。因此，应用程序应该在callback函数中，继续执               行之前获取这些数据时，想要执行的任务（当时没能马上得到这些数据，所以当时无法执行这些任务）
         14.4 Deferred：Twisted使用Deferred对象来管理callback序列
              作为Twisted库的“客户端”，应用程序将一连串函数添加到Deferred对象中，当异步请求的结果准备就绪时，这一连串函数将被按顺序                调用（这一连串函数被称为一个callback序列，或是一条callback链），一起添加的还有另外一连串函数，当异步请求出现错误的时候，               他们将被调用（称作一个errback序列，或是一条errback链）。异步库代码会在结果准备就绪时，调用第一个callback，或是在出现错误               时，调用第一个errback，然后Deferred对象就会将callback或errback的返回结果传递给链中的下一个函数。
               
              Deferred被设计用来使Twisted程序可以无阻塞地等待数据，直到数据准备就绪。为了达到这个目的，Deferred为库与应用程序提供了一              个简单的callback管理接口。库可以通过调用Deferred.callback来把准备就绪的数据传回给应用程序，或者调用Deferred.errback来报              告一个错误。应用程序可以按它们希望的顺序，设置结果处理逻辑，以callback与errback的形式添加到Deferred对象中去。

              使 CPU尽可能的有效率，是Deferred与该问题的其他解决方案背后的基本思路。如果一个任务在等待数据，与其让CPU（以及程序本身！              ）眼巴巴地等着数据（对于进程，这通常叫做“阻塞”），不如让程序同时执行些别的操作，并且同时留意着某些信号――一旦数据准备              就绪，程序就可以（但不是立即――译者注）回到刚才的地方继续执行。

              在Twisted里，一个函数返回一个Deferred对象，意味着它给调用者一个信号：它在等待数据。当数据准备就绪后，程序就会激活那个                  Deferred对象的callback链，来处理数据。



              当有多个addCallback的时候,第一个addCallback(lowerCaseContents)执行后的结果,会当作第二个addCallback(printContents)

         14.5 Reactor模式
              Twisted实现了设计模式中的反应堆（reactor）模式，这种模式在单线程环境中调度多个事件源产生的事件到它们各自的事件处理例程中              去。Twisted的核心就是reactor事件循环。Reactor可以感知网络、文件系统以及定时器事件。它等待然后处理这些事件，从特定于平台                的行为中抽象出来，并提供统一的接口，使得在网络协议栈的任何位置对事件做出响应都变得简单。

              注意：
                  1. Twisted的reactor只有通过调用reactor.run()才启动。
                  2. reactor循环是在其开始的线程中运行，也就是运行在主线程中。
                  3. 一旦启动，reactor就会在程序的控制下（或者具体在一个启动它的线程的控制下）一直运行下去。
                  4. reactor空转时并不会消耗任何CPU的资源。
                  5. 并不需要显式的创建reactor，只需要引入就OK了。（一个程序中只能有一个reactor，并且只要你引入它就相应地创建一个）
                  
                  1. reactor模式是单线程的。
                  2. 像Twisted这种交互式模型已经实现了reactor循环，意味无需我们亲自去实现它。
                  3. 我们仍然需要框架来调用我们自己的代码来完成业务逻辑。
                  4. 因为在单线程中运行，要想跑我们自己的代码，必须在reactor循环中调用它们。
                  5. reactor事先并不知道调用我们代码的哪个函数

                  1. 我们的代码与Twisted代码运行在同一个线程中。
                  2. 当我们的代码运行时，Twisted代码是处于暂停状态的。
                  3. 同样，当Twisted代码处于运行状态时，我们的代码处于暂停状态。
                  4. reactor事件循环会在我们的回调函数返回后恢复运行
15. Python常用模块之sys
              sys模块提供了一系列有关Python运行环境的变量和函数。
              这个模块可供访问由解释器使用或维护的变量和与解释器进行交互的函数。

16. Python常用模块之os
              os模块就是对操作系统进行操作。
              这个模块提供了一种方便的使用操作系统函数的方法。 

17. 递归：在函数内部，调用函数自身的编程技巧称为递归（ recursion）
    严重缺点：相对于普通循环而言，递归运行效率较低，经过很多冗余的计算，递归会消耗大量的调用堆栈。因此，应该尽量用循环代替递归。
             递归深度：
             import sys
             #查看递归深度
             sys.getrecursionlimit()
             #设置最大递归深度,python默认1000，不宜设置的太小，当超过最大深度会报错
             sys.setrecursionlimit(1000)

18. 迭代：利用 for 循环来遍历一个列表（list）或元组（tuple），将值依次取出，这种方法我们称为迭代。

             Python还提供一个 reduce 函数，利用 lambda 匿名函数，一行代码便可以完成阶乘的计算：
             print reduce(lambda x,y: x*y, range(1,11))

19. 进程：
          进程的创建 fork,只有Linux中有，父进程结束和子进程没有任何关系
          multiprocessing模块创建进程,linux 和 Windows 都有，multiprocessing创建的子进程，父进程会在所有子进程结束后才结束
         全局变量在多进程中不共享，在函数里面修改全局变量，需要加global
          p.join([timeout]) 一直等p进程执行完才会向下执行，否则一直阻塞，timeout后无论进程是否执行完，继续向下执行
          terminate 不管任务是否完成，立即终止

          如果要是用pool创建进程，就需要使用multiprocessing.Manager()中的Queue(),而不是multiprocessing.Queue()否则会报错，
          q = Manager().Queue() #使用Manager中的Queue来初始化

20. 线程：
          线程共享全局变量
          acquire(blocking=true,timeout=-1) blocking=true默认阻塞 ，timeout=-1默认永远   等待锁的时间
          创建ThreadLocal对象：local_school = threading.local()

21.scrapy文档笔记:
           21.1：一旦我们调用 Selector.remove_namespaces()（移除命名空间） 方法，所有的节点都可以直接通过他们的名字来访问
                 >>> response.selector.remove_namespaces()
                 >>> response.xpath("//link")
                 
                 为什么命名空间移除操作并不总是被调用，而需要手动调用有疑惑。这是因为存在如下两个原因，按照相关顺序如下：
                 1.移除命名空间需要迭代并修改文件的所有节点，而这对于Scrapy爬取的所有文档操作需要一定的性能消耗
                 2.会存在这样的情况，确实需要使用命名空间，但有些元素的名字与命名空间冲突。尽管这些情况非常少见。

22. <tbody> 元素：
            当您检查网页源码的时候， 其已经不是原始的HTML，而是经过浏览器清理并执行一些Javascript代码后的结果。 Firefox是个典型的例子            ，其会在table中添加 <tbody> 元素。 而Scrapy相反，其并不修改原始的HTML，因此如果在XPath表达式中使用 <tbody> ，您将获取不到            任何数据。

23. scrapy文档笔记：
            1. response.follow(next_page,self.parse): 返回一个Request实例来跟踪链接url。它接受与Request.__init__方法相同的参数，但url                                                       可以是相对URL或scrapy.link.Link对象，不仅是绝对URL。

                          在scrapy1.4版本后才有此方法response.follow(),
                          之前版本应该用  yield scrapy.Reques(next_page, self.parse),用此方法拼接url时可用 
                          next_page = response.urljoin(next_page)
                
              对于<a>元素有一个快捷方式：response.follow自动使用他们的href属性。所以代码可以进一步缩短：
              for a in response.css('li.next a'):
                  yield response.follow(a, callback=self.parse)


            2. 命令写入json文件编码问题： scrapy runspider quotes_spider.py -o quotes.json -s  FEED_EXPORT_ENCODING=utf-8
            
            3. scrapy crawl spidername     spidername为 name = "spidername"
            
            4. parse(self, response) 中输出日志：self.log("Save the file %s" % filename)
          
            5. scrapy crawl quotes -o quotes.json ：
                           由于历史原因，Scrapy附加到给定文件而不是覆盖其内容。如果您在第二次执行该命令两次而未                                                   删除该文件的情况下运行该命令，则最终会生成一个损坏的JSON文件。
               scrapy crawl quotes -o quotes.jl ：
                           它没有相同的JSON问题。另外，由于每条记录都是一条独立的行，因此您可以处理大文件而不必将所有内容都放在内存中                            ，而像JQ这样的工具可以帮助在命令行执行该操作。
            
            6. -a 运行时，可以使用该选项向蜘蛛提供命令行参数：scrapy crawl quotes -o quotes-humor.json -a tag=humor
                           这些参数被传递给蜘蛛的__init__方法，并默认成为蜘蛛属性。
 
            7. parse_start_url(response) ：这个方法被称为start_urls响应。它允许解析初始响应，
                          当start_url的请求返回时，该方法被调用。 该方法分析最初的返回值并必须返回一个 Item对象或者 一个 Request 对象                          或者 一个可迭代的包含二者对象。

            8. SitemapSpider：
                                from scrapy.spiders import SitemapSpider

                                class MySpider(SitemapSpider):
                                    sitemap_urls = ['http://www.example.com/sitemap.xml']

                                    def parse(self, response):
                                        pass # ... scrape item here ...
            9. 要实际提取文本数据，您必须调用选择器.extract() 方法,
               如果你只想提取第一个匹配的元素，你可以调用选择器 .extract_first(),如果没有找到元素它会返回None，默认返回值可以作为参数               提供，用来代替None：response.xpath("//div[@id='not-exists']/text()").extract_first(default='not-found')
          
            10. enumerate()说明:
                                  enumerate()是python的内置函数
                                  enumerate在字典上是枚举、列举的意思
                                  对于一个可迭代的（iterable）/可遍历的对象（如列表、字符串），enumerate将其组成一个索引序列，利用它可                                     以同时获得索引和值
                                  enumerate多用于在for循环中得到计数
                                  例： list1 = ["这", "是", "一个", "测试"]
                                       for index, item in enumerate(list1, 1):
                                           print index, item
                                        >>>
                                        1 这
                                        2 是
                                        3 一个
                                        4 测试

 
        
                 
                                



            

              


















   